---
breadcrumb: PCF Metrics Documentation
title: Sizing PCF Metrics for Your System 
owner: PCF Metrics
list_style_none: true
---

This topic describes how operators configure Pivotal Cloud Foundry (PCF) Metrics depending on their deployment size. 
Operators can use these procedures to optimize PCF Metrics for high capacity or to reduce resource usage for smaller deployment sizes.

Although [Configure Resources](#resource-config) in the installation procedure shows sample default configurations,
if you know in advance how many apps your deployment will be running, 
use the resource scaling information in this topic to size your first deployment.
Alternatively, after your deployment has been running for a while, use the information in this topic to scale your running deployment. 

If you are not familiar with the PCF Metrics components, review [PCF Metrics Product Architecture](./architecture.html) before reading this topic.

For how to configure resources during installation, see [Configure Resources](./installing.html#resource-config).
For how to configure resources for a running deployment, see the procedures below:

+ [Procedure for Scaling the Metrics Datastore](#metrics-procedures)
+ [Procedure for Scaling the Log Datastore](##log-procedures)
+ [Procedure for Scaling the Temporary Datastore](#temp-procedures)
+ [Procedure for Scaling the Ingestor](#ingestor-procedures)
+ [Procedure for Scaling the Logqueues](##logqueue-procedures)

##<a id='configs-by-size'></a> Suggested Sizing by Deployment Size

Use the following tables as a guide for configuring resources for your deployment. 

Estimate the size of your deployment according to how many apps are expected to be deployed.

<table style='nice'>
   <tr><th>Size</th><th>Purpose</th><th>Approximate number of apps</th></tr>
   <tr><td><a href="#small">Small</a></td><td>Test use</td><td>100</td></tr>
   <tr><td><a href="#medium">Medium</a></td><td>Production use</td><td>5,000</td></tr>
   <tr><td><a href="#large">Large</a></td><td>Production use</td><td>15,000</td></tr>
</table>

   
<p class="note"><strong>Note</strong>: The values in the tables below do not account for additional load on MySQL from custom metrics. 
Pivotal recommends you start with the one of the following configurations and scale up as necessary 
by following the steps in <a href="#metrics-datastore">Configuring the Metrics Datastore</a>.</p>

###<a id='small'></a>Deployment Resources for a Small Deployment

This table lists the resources you need to configure for a small deployment, about 100 apps.

<table>
  <tr><th>Job</th><th>Instances</th><th>Persistent Disk Type</th><th width=40%>VM Type</th></tr>
  <tr><td>Elasticsearch Master</td><td>1</td><td>10&nbsp;GB</td><td>small (cpu:&nbsp1, ram:&nbsp;2&nbsp;GB, disk:&nbsp;8&nbsp;GB)</td></tr>
  <tr><td>Elasticsearch Data</td><td>1</td><td>10&nbsp;GB</td><td>small (cpu:&nbsp1, ram:&nbsp;2&nbsp;GB, disk:&nbsp;8&nbsp;GB)</td></tr>
  <tr><td>Redis</td><td>1</td><td>10&nbsp;GB</td><td>micro (cpu:&nbsp1, ram:&nbsp;1&nbsp;GB, disk:&nbsp;8&nbsp;GB)</td></tr>
  <tr><td>MySQL Server</td><td>1 (not configurable)</td><td>10&nbsp;GB</td><td>small (cpu:&nbsp1, ram:&nbsp;2&nbsp;GB, disk:&nbsp;8&nbsp;GB)</td></tr>
</table>

###<a id='medium'></a>Deployment Resources for a Medium Deployment

This table lists the resources you need to configure for a medium deployment, about 5000 apps.

<table>
  <tr><th>Job</th><th>Instances</th><th>Persistent Disk Type</th><th width=40%>VM Type</th></tr>
  <tr><td>Elasticsearch Master</td><td>1</td><td>10&nbsp;GB</td><td>small (cpu: 1, ram: 2&nbsp;GB, disk: 8&nbsp;GB)</td></tr>
  <tr><td>Elasticsearch Data</td><td>5</td><td>200&nbsp;GB</td><td>small.disk (cpu: 1, ram: 2&nbsp;GB, disk: 16&nbsp;GB)</td></tr>
  <tr><td>Redis</td><td>1</td><td>10&nbsp;GB</td><td>small.disk (cpu: 1, ram: 2&nbsp;GB, disk: 16&nbsp;GB)</td></tr>
  <tr><td>MySQL Server</td><td>1 (not configurable)</td><td>500&nbsp;GB</td><td>medium (cpu: 2, ram: 4&nbsp;GB, disk: 8&nbsp;GB)</td></tr>
</table>

###<a id='large'></a>Deployment Resources for a Large Deployment

This table lists the resources you need to configure for a large deployment, about 15,000 apps.

<table>
  <tr><th>Job</th><th>Instances</th><th>Persistent Disk Type</th><th width=40%>VM Type</th></tr>
  <tr><td>Elasticsearch Master</td><td>1</td><td>10&nbsp;GB</td><td>small (cpu: 1, ram: 2&nbsp;GB, disk: 8&nbsp;GB)</td></tr>
  <tr><td>Elasticsearch Data</td><td>10</td><td>500&nbsp;GB</td><td> large (cpu: 2, ram: 8&nbsp;GB, disk: 16&nbsp;GB)</td></tr>
  <tr><td>Redis</td><td>1</td><td>10&nbsp;GB</td><td>large (cpu: 2, ram: 8&nbsp;GB, disk: 16&nbsp;GB)</td></tr>
  <tr><td>MySQL Server</td><td>1 (not configurable)</td><td>2 &nbsp;TB</td><td>large (cpu: 2, ram: 8&nbsp;GB, disk: 16&nbsp;GB)</td></tr>
</table>



##<a id='metrics-datastore'></a> Scale the Metrics Datastore


PCF Metrics stores metrics in a single MySQL node.
For PCF deployments with high app logs load, you can add memory and persistent disk to the MySQL server node.

###<a id='metrics-considerations'></a> Considerations for Scaling the Metrics Datastore

While the default configurations above are a good starting point for your MySQL server node, 
they do not take into account the additional load from custom metrics.
Pivotal recommends evaluating performance over a period of time and scaling upwards as necessary.
As long as persistent disk is scaled up, you won't not lose any data from scaling.


###<a id='metrics-procedures'></a> Procedure for Scaling

After determining the amount of memory and persistent disk required for your MySQL server node, do the following to scale up the MySQL server node: 

*Q: Is this topic only about scaling the MySQL Server VM? Or all the VMs on the Resource Config page? 
Q: How do customers determine the memory and persistent disk required? Is it as simple as looking at the tables above?* 

To scale up the MySQL server node, do the following:

1. Determine how much memory and persistent disk are required for the MySQL server node.
1. Navigate to the Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Enter the values for the Persistent Disk Type and VM type.
1. Click **Save**.

<p class="note warning"><strong>WARNING!</strong> If you are using PCF v1.9.x and earlier,
   there might be issues Ops Manager BOSH Director using persistent disks larger than 2&nbsp;TB.</p>

##<a id='log-datastore'></a> Scaling the Log Datastore

PCF Metrics uses Elasticsearch to store logs.
Each Elasticsearch node contains multiple shards of log data, divided by time slice.

###<a id='log-considerations'></a> Considerations for Scaling

The default configurations outlined above are a good starting point for your Elasticsearch Master and Data nodes.
For more precision, perform the following calculations, using your specific log loads.

First, you'll need to determine how many logs the apps in your deployment emit and the average size of each log.
If your average log size is x, average log rate is r logs/hour, the persistent disk size of a single ES data node is y,
and number of ES data nodes is n, then n = r * 336 * x / y. 
This assumes the retention period for logs is 2 weeks (and there are 336 hours in 2 weeks),
and that x and y have the same units.
You can use this formula to scale n and y as you see fit.
If you want to achieve high availability for logs, then double the number of ES data nodes.

###<a id='log-procedures'></a> Procedure for Scaling

<p class="note warning">
<strong>WARNING!</strong> If you modify the number of Elasticsearch instances,
               the Elasticsearch cluster temporarily enters an unhealthy period during which
               it does not ingest any new logs data, due to shard allocation.</p>

After determining the number of Elasticsearch nodes needed for your deployment, perform the following steps to scale your nodes:

1. Navigate to the Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Locate the **Elasticsearch Data** job and select the dropdown menu under **Instances** to change the number of instances. Need to modify to include changing the persistent disk and VM type.
	<br>
	<%= image_tag('elasticsearch.png') %>
1. Click **Save**.  

##<a id='temp-datastore'></a> Scale the Temporary Datastore (Redis)

PCF Metrics uses Redis to temporarily store ingested data from the Loggregator Firehose
as well as cache data queried by the Metrics API.
The former use case is to prevent major metrics and logs loss when the data stores (Elasticsearch and MySQL) are unavailable.
The latter is to potentially speed up frontend queries. See [PCF Metrics Product Architecture](./architecture.html) for more information.

###<a id='temp-considerations'></a> Considerations for Scaling

The default Redis configuration specified in the tables above that fits your deployment size should work for most cases.
Redis stores all data in memory, so if your deployment size requires it,
you can also consider scaling up the RAM for your Redis instance(s).
You can additionally increase the number of Redis instances to 2 if you need HA behavior when Redis upgrades.

###<a id='temp-procedures'></a> Procedure for Scaling

Follow these steps to configure the size of the Redis VM for the temporary datastore based on your calculations.

<p class="note"><strong>Note</strong>: In the case that the temporary datastore becomes full,
Redis uses the <code>volatile-ttl</code> eviction policy to continue storing incoming logs.
For more information, see the <i>Eviction policies</i> section of <a href="https://redis.io/topics/lru-cache">Using Redis as an LRU cache</a>.</p>

1. Navigate to the Ops Manager Installation Dashboard and click the **Redis** tile.

1. From the **Settings** tab, click **Resource Config**. 

1. In the **Dedicated Node** row, under **VM Type**, select an option with enough **ram** for your deployment size.
   Or, select the number of Redis instances you want under **Instances**.

1. Click **Save**. 

##<a id='ingestor'></a> Scale the Ingestor

PCF Metrics deploys the Ingestor as an app within PCF.
The Ingestor consumes logs and metrics from the Loggregator Firehose, sending metrics and logs to their respective Logqueue apps.
To customize PCF Metrics for high capacity, you can scale the number of Ingestor app instances and increase the amount of memory per instance.

###<a id='ingestor-considerations'></a> Considerations for Scaling the Ingestor

Because apps emit logs at different volumes and frequencies, you should not scale the Ingestor by matching the number of Ingestor instances to the number of app instances in your deployment.

Because Ingestor performance is affected by Loggregator performance, it can be difficult to determine in advance the proper configuration.
Because of the ease in scaling these components,
Pivotal recommends starting with the configuration in the table above (for your deployment size)
and then evaluating performance over a period of time and scaling upwards if performance degrades. 

###<a id='ingestor-procedures'></a> Procedure for Scaling

<p class="note warning"><strong>WARNING! </strong> If you decrease the number of Ingestor instances, you might lose data currently being processed on the instances you eliminate.</p>

After determining the number of Ingestor app instances needed for your deployment, 
1perform the following steps to scale the Ingestor:

1. Target your Cloud Controller with the Cloud Foundry Command Line Interface (cf CLI).
   If you have not installed the cf CLI, see [Installing the cf CLI](http://docs.pivotal.io/pivotalcf/cf-cli/install-go-cli.html).
	<pre class="terminal">
	$ cf api api.YOUR-SYSTEM-DOMAIN
	Setting api endpoint to api.YOUR-SYSTEM-DOMAIN...
	OK
	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	Not logged in. Use 'cf login' to log in.
	</pre>

1. Log in with your UAA administrator credentials.
   To retrieve these credentials, navigate to the **Pivotal Elastic Runtime** tile in the Ops Manager Installation Dashboard and click **Credentials**.
   Under **UAA**, click **Link to Credential** next to **Admin Credentials** and record the password.
	<pre class="terminal">
	$ cf login
	API endpoint: <span>https:</span>//api.YOUR-SYSTEM-DOMAIN

	Email> admin
	Password>
	Authenticating...
	OK

1. When prompted, target the `metrics` space.
	<pre class="terminal">
	Targeted org system

	Select a space (or press enter to skip):
	<span>1</span>. system
	<span>2</span>. notifications-with-ui
	<span>3</span>. autoscaling
	<span>4</span>. metrics

	Space> 4
	Targeted space metrics

	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	User:           admin
	Org:            system
	Space:          metrics
	</pre>

1. Scale your Ingestor app to the desired number of instances:
	<pre class="terminal">$ cf scale metrics-ingestor -i INSTANCE-NUMBER</pre>
1. Evaluate the CPU and memory load on your Ingestor instances:
	<pre class="terminal">
	$ cf app metrics-ingestor
	Showing health and status for app metrics-ingestor in org system / space metrics as admin...
	OK
	<br>
	requested state: started
	instances: 1/1
	usage: 1G x 1 instances
	urls: 
	last uploaded: Sat Apr 23 16:11:29 UTC 2016
	stack: cflinuxfs2
	buildpack: binary_buildpack 
	<br>	
	     state     since                    cpu    memory        disk           details
	<span>#</span>0   running   2016-07-21 03:49:58 PM   2.9%   13.5M of 1G   12.9M of 1G
	</pre>

	If your average memory usage exceeds 50% or your CPU consistently averages over 85%, add more instances with `cf scale metrics-ingestor -i INSTANCE-NUMBER`.
	<br><br>
	In general, you should scale the Ingestor app by adding additional instances. However, you can also scale the Ingestor app by increasing the amount of memory per instance:

	<pre class="terminal">$ cf scale metrics-ingestor -m NEW-MEMORY-LIMIT</pre>

	For more information about scaling app instances, see [Scaling an Application Using cf scale](http://docs.pivotal.io/pivotalcf/devguide/deploy-apps/cf-scale.html).

##<a id='logqueue'></a> Scale the Logqueues

PCF Metrics deploys a MySQL Logqueue and an Elasticsearch Logqueue as apps within PCF.
The MySQL logqueue consumes metrics from the Ingestor and forwards them to MySQL.
The Elasticsearch logqueue consumes logs from the Ingestor and forwards them to Elasticsearch.
To customize PCF Metrics for high capacity, you can scale the number of Logqueue app instances and increase the amount of memory per instance.

###<a id='logqueue-considerations'></a> Considerations for Scaling

The number of MySQL and Elasticsearch logqueues needed is dependent on the frequency of logs and metrics being forwarded by the Ingestor.
As a general rule, for every 45,000 logs per minute, add 2 Elasticsearch logqueues.
For every 17,000 metrics per minute, add 1 MySQL Logqueue.
This is a general estimate and you might need fewer instances depending on your deployment.
To optimize resource allocation, provision fewer instances initially and increase instances until you achieve desired performance.

###<a id='logqueue-procedures'></a> Procedures for Scaling

To modify your Elasticsearch Logqueue app instances, you must first target your Cloud Controller,
log in with your UAA administrator credentials,
and target the `metrics` space by following steps 1-3 in the [previous section](#ingestor-procedures).

To scale your Logqueue app instances, perform the following command:
<pre class="terminal">$ cf scale elasticsearch-logqueue -i INSTANCE-NUMBER</pre>

To scale the memory limit per Logqueue app instance, perform the following command:
<pre class="terminal">$ cf scale elasticsearch-logqueue -m NEW-MEMORY-LIMIT</pre>

To modify your MySQL Logqueue app instances, you must first target your Cloud Controller,
log in with your UAA administrator credentials, and target the `metrics` space by following steps 1-3 in the [previous section](#ingestor-procedures).

To scale your Logqueue app instances, perform the following command:
<pre class="terminal">$ cf scale mysql-logqueue -i INSTANCE-NUMBER</pre>

To scale the memory limit per Logqueue app instance, perform the following command:
<pre class="terminal">$ cf scale mysql-logqueue -m NEW-MEMORY-LIMIT</pre>

<p class="note warning">
<strong>WARNING! </strong> If you decrease the number of Logqueue instances,
you might lose data currently being processed on the instances you eliminate.</p>


*TBD: This is another table that Judy will add that applies to Elasticsearch and Logqueues too.*


| Item | Small | Medium | Large |
|-----------------------------|--------------------------------|-------------------------------|--------------------------------|
|ingestor instance count      | Equal to the number of dopplers|Equal to the number of dopplers|Equal to the number of dopplers |
|mysql logqueue instance count| 1                              | 1                             |2                               |
|es logqueue instance count   | 1                              | 2                             | 3                              |
|metrics api instance count   | 1                              | 2                             | 2                              |
